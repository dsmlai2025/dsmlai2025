## Data_Engineering

NanoDegree Course Details: [Udacity Data Engineer Nanodegree](https://www.udacity.com/course/data-engineer-nanodegree--nd027)

| Project Folder                                      | Project                                       | Detailed Description                                                                                                                                                                                                              | Complete        |
|----------------------------------------------------|-----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| **[Project 1 - PostgreSQL](1_Project_Data_Modeling_with_Postgres)**             | Model user activity data for Sparkify app using a Relational database created with PostgreSQL  | - Designed a relational database schema with Fact and Dimension tables (Star Schema) <br> - Developed an ETL pipeline to optimize queries to analyze user listening behavior                                                   | :heavy_check_mark: |
| **[Project 2 - Cassandra](2_Project_Data_Modeling_with_Apache_Cassandra)**      | Model user activity data for Sparkify app using NoSQL database (Apache Cassandra)               | - Created denormalized tables optimized for transactional writes on user sessions <br> - Implemented a NoSQL data model suited to the app's needs                                                                              | :heavy_check_mark: |
| **[Project 3 - Data Warehouse w AWS Redshift](3_Project_Cloud_Data%20Warehouse)** | Build a cloud data warehouse on AWS Redshift                                               | - Created Redshift cluster with IAM Roles and Security Groups <br> - Built an ETL pipeline to load data from S3 into star schema <br> - Enabled analytics team to query efficiently                                               | :heavy_check_mark: |
| **[Project 4 - Data Lake w Apache Spark](4_Project_Data%20Lake_Spark)**           | Build an ETL data lake pipeline using Apache Spark                                         | - Created an EMR Hadoop cluster <br> - Developed scalable ETL pipelines to process datasets on S3 via Spark <br> - Employed efficient partitioning and parquet file format                                                      | :heavy_check_mark: |
| **[Project 5 - Airflow Pipelines](5_Project_Data_Pipelines_w_Apache_Airflow)**     | Automate Sparkify data infrastructure with Apache Airflow                                | - Created custom Airflow operators for data staging, data warehouse loading, and data quality checks <br> - Automated and monitored end-to-end data pipelines                                                                    | :heavy_check_mark: |
| **[Project 6 - Capstone Project]()**                                                  | Modernize data infrastructure using Kaggle dataset                                      | - Developed a production-ready ETL pipeline to load, process, and store analytics-ready parquet datasets on S3 using Spark                                                                                                      | :heavy_check_mark: |
